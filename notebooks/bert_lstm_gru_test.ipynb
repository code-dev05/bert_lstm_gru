{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertModel\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_LSTM_GRU(nn.Module):\n",
    "    def __init__(self, bert_model, hidden_dim, embedding_dim):\n",
    "        super(BERT_LSTM_GRU, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.bert.requires_grad_(False)\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, batch_first=True).to(torch.float32)\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, batch_first=True).to(torch.float32)\n",
    "        self.lm_head = nn.Linear(hidden_dim, embedding_dim).to(torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            embedding = self.bert(x).last_hidden_state\n",
    "        x, _ = self.lstm(embedding)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_LSTM_GRU(bert_model, 512, tokenizer.vocab_size)\n",
    "checkpoint = torch.load(\"../models/model_epoch2_step20000.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=50, device=device):\n",
    "    model.eval()\n",
    "\n",
    "    tokens = tokenizer.encode(prompt, add_special_tokens=True, return_tensors='pt').to(device)\n",
    "\n",
    "    generated_tokens = tokens.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits = model(generated_tokens)\n",
    "\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "\n",
    "            generated_tokens = torch.cat([generated_tokens, next_token], dim=1)\n",
    "\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    generated_text = tokenizer.decode(generated_tokens.squeeze(0), skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, there was a big dog named Max. Max had a red collar that he wore every day. He loved to play and run in the park with his friends.\"\n",
    "print(generate_text(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "reference = [prompt.lower().split()]\n",
    "candidate = generate_text(model, tokenizer, prompt).split()\n",
    "\n",
    "meteor_score(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "reference = prompt\n",
    "candidate = generate_text(model, tokenizer, prompt)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "scores = scorer.score(reference, candidate)\n",
    "\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
